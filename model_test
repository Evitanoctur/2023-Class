import torch
from transformers import BertModel, BertTokenizer, BertConfig
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
config = BertConfig.from_pretrained('bert-base-uncased')
config.update({'output_hidden_states':True}) 
model = BertModel.from_pretrained("bert-base-uncased",config=config)
from torch.utils.data import DataLoader,Dataset
import os
import re
import sklearn
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from random import sample
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from transformers import BertModel, BertTokenizer
from tqdm import tqdm
data_base_path = r"./aclImdb_v1.tar_files/aclImdb"
model_path = r"./mode"
class ImdbDataset(Dataset):
    def __init__(self,mode,testNumber=10000,validNumber=5000):
        super(ImdbDataset,self).__init__()
        text_path =  [os.path.join(data_base_path,i)  for i in ["test/neg","test/pos"]]
        traintext_path = [os.path.join(data_base_path,i)  for i in ["train/neg","train/pos"]]
        if mode=="train":
            self.total_file_path_list = []
            for i in traintext_path:
                self.total_file_path_list.extend([os.path.join(i,j) for j in os.listdir(i)])
        if mode=="test":
            self.total_file_path_list = []
            for i in text_path:
                self.total_file_path_list.extend([os.path.join(i,j) for j in os.listdir(i)])
                self.total_file_path_list=sample(self.total_file_path_list,testNumber)
        if mode=="valid":
            self.total_file_path_list = []
            for i in text_path:
                self.total_file_path_list.extend([os.path.join(i,j) for j in os.listdir(i)])
                self.total_file_path_list=sample(self.total_file_path_list,validNumber)
    def tokenize(self,text):
        fileters = ['!','"','#','$','%','&','\(','\)','\*','\+',',','-','\.','/',':',';','<','=','>','\?','@'
            ,'\[','\\','\]','^','_','`','\{','\|','\}','~','\t','\n','\x97','\x96','”','“',]
        text = re.sub("<.*?>"," ",text,flags=re.S)
        text = re.sub("|".join(fileters)," ",text,flags=re.S)
        return text
    def __getitem__(self, idx):
        cur_path = self.total_file_path_list[idx]
        cur_filename = os.path.basename(cur_path)
        labels = []
        sentences = []
        if int(cur_filename.split("_")[-1].split(".")[0]) <= 5 :
            label = 0
        else:
            label = 1
        labels.append(label)
        text = self.tokenize(open(cur_path,encoding='UTF-8').read().strip())
        sentences.append(text)
        return sentences,labels
    def __len__(self):
        return len(self.total_file_path_list)
class BertClassificationModel(nn.Module):
    def __init__(self,hidden_size=768):
        super(BertClassificationModel, self).__init__()
        model_name = 'bert-base-uncased'
        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)
        self.bert = BertModel.from_pretrained(pretrained_model_name_or_path=model_name)

        for p in self.bert.parameters(): 
                p.requires_grad = False
        self.fc = nn.Linear(hidden_size,2)
    def forward(self, batch_sentences):
        sentences_tokenizer = self.tokenizer(batch_sentences,
                                             truncation=True,
                                             padding=True,
                                             max_length=512,
                                             add_special_tokens=True)
        input_ids=torch.tensor(sentences_tokenizer['input_ids']) 
        attention_mask=torch.tensor(sentences_tokenizer['attention_mask']) 
        bert_out=self.bert(input_ids=input_ids,attention_mask=attention_mask) 
        last_hidden_state =bert_out[0] # [batch_size, sequence_length, hidden_size] 
        bert_cls_hidden_state=last_hidden_state[:,0,:] 
        fc_out=self.fc(bert_cls_hidden_state) 
        return fc_out
testNumber=2000
validNumber=500
batchsize=50
testDatas = ImdbDataset(mode="test",testNumber=testNumber) 
validDatas = ImdbDataset(mode="valid",validNumber=validNumber) 
val_loader = torch.utils.data.DataLoader(validDatas, batch_size=batchsize, shuffle=False)   
test_loader = torch.utils.data.DataLoader(testDatas, batch_size=batchsize, shuffle=False)   
import pickle
import evaluate
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)
num_a = 0
num_b = 0
auc = 0
model.eval()  # 不启用 BatchNormalization 和 Dropout，保证BN和dropout不发生变化,主要是在测试场景下使用；
for j, (data,labels) in enumerate(test_loader, 0):
    print('testing......')
    output = model(data[0])
    out = output.argmax(dim=1)
    num_a += (out == labels[0]).sum().item()
    num_b += f1_score(labels[0],out)
print('Accuracy:', num_a / testNumber)
print('f1score:',num_b / (testNumber/batchsize))
